---
layout:	    post
title:      "Understanding Global Average Pooling"
subtitle:   "不一样的GAP"
date:       2017-07-16
author:     "Jinquan"
header-img: "img/post-dynet-insight-bg.jpg"
tags:
    - 深度学习
    - Trick
---

> Golbal Average Pooling 第一次出现在论文Network in Network中，后来又很多工作延续使用了GAP，实验证明：Global Average Pooling确实可以提高CNN效果。

### Traditional Pooling Methods

要想真正的理解Global Average Pooling，首先要了解深度网络中常见的pooling方式，以及全连接层。

众所周知CNN网络中常见结构是：卷积、池化和激活。卷积层是CNN网络的核心，激活函数帮助网络获得非线性特征，而池化的作用则体现在降采样：保留显著特征、降低特征维度，增大kernel的感受野。深度网络越往后面越能捕捉到物体的语义信息，这种语义信息是建立在较大的感受野基础上。已古人的例子来做解释，想必大家都知道盲人摸象这个成语的来历，每个盲人只能触摸到大象的一部分，也就是只能获得local response，基于这些local response，盲人们很难猜对他们到底在摸什么。即使是一个明眼人，眼睛紧贴这大象后背看，也很难猜到看的是什么。这个例子告诉我们局部信息很难提供更高层的语义信息，因此对feature map降维，进而提高后面各层kernel的感受野是一件很重要的事情。另外一点值得注意：pooling也可以提供一些旋转不变性。

常见的Pooling方式有以下几种：

1. Max pooling: 简言之就是在窗口范围之内保留最大值，我们可以想象同一副图像，即使经过了一定角度（这个角度通常不是很大）的旋转，那么被激活的位置仍然有可能是未旋转前的位置，这也就是CNN具有一定旋转不变性的原因
2. Mean pooling：在窗口范围之内取平均值

---

### Fully Connected layer

很长一段时间以来，全连接网络一直是CNN分类网络的标配结构。一般在全连接后会有激活函数来做分类，假设这个激活函数是一个多分类softmax，那么全连接网络的作用就是将最后一层卷积得到的feature map stretch成向量，对这个向量做乘法，最终降低其维度，然后输入到softmax层中得到对应的每个类别的得分。

全连接层如此的重要，以至于全连接层过多的参数重要到会造成过拟合，所以也会有一些方法专门用来解决过拟合，比如dropout。

---

### Global Average Pooling

有了上面的基础，再来看看global average poolilng。既然全连接网络可以使feature map的维度减少，进而输入到softmax，但是又会造成过拟合，是不是可以用pooling来代替全连接。

答案是肯定的，*Network in Network*工作使用GAP来取代了最后的全连接层，直接实现了降维，更重要的是极大地减少了网络的参数(CNN网络中占比最大的参数其实后面的全连接层)。Global average pooling的结构如下图所示:

![FCN-architecture](G:\sunalbert.github.io\img\in-post\post-gap\gap.JPG)

<small class="img-hint">GAP结构示意图</small>

每个讲到全局池化的都会说GAP就是把avg  pooling的窗口大小设置成feature map的大小，这虽然是正确的，但这并不是GAP内涵的全部。**GAP的意义是对整个网络从结构上做正则化防止过拟合。**既要参数少避免全连接带来的过拟合风险，又要能达到全连接一样的转换功能，怎么做呢？直接从feature map的通道上下手，如果我们最终有1000类，那么最后一层卷积输出的feature map就只有1000个channel，然后对这个feature map应用全局池化，输出长度为1000的向量，这就相当于**剔除了全连接层黑箱子操作的特征，直接赋予了每个channel实际的类别意义**。

实验证明，这种方法是非常有效的，



这样做还有另外一个好处：不用在乎网络输入的图像尺寸。